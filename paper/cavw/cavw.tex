\documentclass[VANCOUVER,STIX1COL]{WileyNJD-v2}

\articletype{Research Article}%

\received{26 April 2022}
\revised{6 June 2022}
\accepted{6 June 2022}

\raggedbottom

% ========================= PACKAGES ============================
\usepackage{graphicx}
%\usepackage{cite}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{xspace}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot}
\def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot}
\def\Ie{\emph{I.e}\onedot}
\def\cf{\emph{c.f}\onedot}
\def\Cf{\emph{C.f}\onedot}
\def\etc{\emph{etc}\onedot}
\def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\usepackage{xcolor}
\usepackage{multirow}
\usepackage{threeparttable}
%\graphicspath{{assets/}}                              % set paths
%\DeclareGraphicsExtensions{.pdf,.jpeg,.png}      % set extensions

\begin{document}

\title{Engagement Estimation of the Elderly from Wild Multiparty Human-Robot Interaction}%\protect\thanks{This is an example for title footnote.}}

\author[1]{Zhijie Zhang}
\author[1]{Jianmin Zheng}
\author[2]{Nadia Magnenat Thalmann}
\authormark{AUTHOR ONE \textsc{et al}}

\address[1]{\orgdiv{School of Computer Science and Engineering}, \orgname{Nanyang Technological University}, \country{Singapore}}
\address[2]{\orgdiv{MIRALab – CUI – University of Geneva}, \orgaddress{\state{Geneva}, \country{Switzerland}}}


%\author[1]{Author One*}
%\author[1]{Author Two}
%\author[2]{Author Three}
%\authormark{AUTHOR ONE \textsc{et al}}

%\address[1]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}
%\address[2]{\orgdiv{Org Division}, \orgname{Org Name}, \orgaddress{\state{State name}, \country{Country name}}}

\corres{*Corresponding author name. \email{authorone@gmail.com}}

\presentaddress{This is sample for present address text}


% ==================== ABSTRACT & KEYWORDS ======================
\abstract[Abstract]{The use of social robots in healthcare systems or nursing homes to assist the elderly and their caregivers will be becoming common, where robots' understanding of engagement of the elderly is important. Traditional engagement estimation often requires expert involvement in a controlled dyadic interaction environment. In this paper, we propose a supervised machine learning method to estimate the engagement state of the elderly in a multiparty human-robot interaction (HRI) scenario from the real-world video recording as input. The method is built upon the basic concept of engagement in geriatric psychiatry and HRI video representations. It adapts pre-trained models to extract behavior, affective and visual signals to form the multi-modal features. These features are then fed into a neural network made of a self-attention mechanism and average pooling for individual learning, a graph attention network for group learning and a fully connected layer to estimate the engagement. We tested the proposed method using 43 wild multiparty elderly-robot interaction videos. The experimental results show that our method is capable of detecting the key participants and estimating the engagement state of the elderly effectively. Also our study demonstrates that the signals from side-participants in the main interaction group considerably contribute to the engagement estimation of the elderly in the multiparty elderly-robot interaction.}

\keywords{Human-robot interaction, Engagement estimation, Affective computing, Multiparty, Machine learning}

%\jnlcitation{\cname{%
%\author{Williams K.},
%\author{B. Hoskins},
%\author{R. Lee},
%\author{G. Masato}, and
%\author{T. Woollings}} (\cyear{2016}),
%\ctitle{A regime analysis of Atlantic winter jet variability applied to evaluate HadGEM3-GC2}, %\cjournal{Q.J.R. Meteorol. Soc.}, \cvol{2017;00:1--6}.}

\jnlcitation{\cname{%
}}

\maketitle

%\footnotetext{\textbf{Abbreviations:} ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor}



% ======================= INTRODUCTION ==========================
\section{Introduction}
\label{s:Introduction}

This paper considers the problem of estimating the engagement of the elderly in wild multiparty human-robot interaction (HRI). With the advance of social robots, deploying robots in the healthcare becomes a possible solution to providing round-the-clock medical and psychological care to the elderly, especially the people with dementia (PwD), and supporting their caregivers as well~\cite{Ghafurian2021Social,Perugia2020ENGAGEDEM}. Natural elderly-robot interaction helps make the robot a good companion for the elderly who usually experience declines in physical and cognitive capacities. This has great impact since the proportion of people aged 60 years and older in the world will nearly double from 12\% in 2015 to 22\% in 2050 according to the World Health Organization~\cite{WHO2021Ageing}.

During the elderly-robot interaction, if the robot can recognize the engagement state of the elderly, it helps the robot to respond to the elderly properly to maintain long-term interaction or to produce appropriate social behavior for the elderly to feel a sense of belonging. Here engagement refers to the inner state of a participant attributing to being together with the other participants and continuing the interaction~\cite{Poggi2013Mind}. Many studies have shown that the engagement plays an important role in both human-human interaction (HHI) and human-robot interaction~\cite{Jones2018Engagement}.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.95\linewidth]{assets/interaction_sample.pdf}
  \caption{Four interaction sessions with five frames from the video recordings of real-world multiparty elderly-robot interaction demonstrate conversation dynamics (from one to more participants) and unconstrained environment (open space and free-moving background people). The videos are recorded from the robot ego-view, and $t = [100, ..., 4000]$ denote five time stamps.}
  \label{f:interaction_sample}
\end{figure}

Engagement estimation is kind of affective computing and behavior recognition, and it goes further to probe the inner intention behind the apparent behavior and emotion. Many methods have been developed to estimate engagement in various scenarios such as general HRI~\cite{Salam2017Fully,Celiktutan2019Multimodal, BenYoussef2019Early,Saleh2021Improving}, museum tour guide~\cite{DelDuchetto2020Are}, classroom or distance learning~\cite{BenEliyahu2018Investigating,Zhu2020Multirate,Rudovic2019Personalized,Sumer2021Multimodal, Monkaresi2017Automated, Abedi2021Affectdriven,Gao2020NGage}, and healthcare~\cite{Anagnostopoulou2021Engagement,Jain2020Modeling,Jones2018Engagement,Steinert2020Engagement}. Conventional approaches use nonverbal cues such as proxemics, body pose, gaze patterns, facial expressions, and context information to build engagement estimation classifiers. Deep learning approaches have also been developed for engagement estimation~\cite{DelDuchetto2020Are,Zhu2020Multirate,Guhan2020ABCNet,Saleh2021Improving,Sumer2021Multimodal,Anagnostopoulou2021Engagement}. However, most previous work assumes that the interaction is in a laboratory environment or a dyadic  situation. When the research is expanded to special populations and more complex circumstances as this paper is (see Fig.~\ref{f:interaction_sample} for example), not much work has been done before. This may be in part due to the following challenges:
\begin{enumerate}
  \item[C1] The non-verbal signals from \textbf{the elderly} alter in facial shape and patterns of body behaviors along with aging~\cite{Guo2013Facial,Folster2014Facial}. This challenges the conventional computer vision approaches in accurately estimating engagement state.
  \item[C2] From dyadic to \textbf{multiparty HRI}, understanding the dynamics and stability of the interaction becomes more complicated.
  \item[C3] In unconstrained \textbf{wild} space, moving people, bad lighting, confusing objects, \etc make it difficult to interpret the complex environment.
\end{enumerate}

To tackle these challenges, in this paper we propose a supervised learning method for estimating engagement from real-world multiparty elderly-robot interaction. It takes video sequences as input and outputs the estimated engagement state. Fig.~\ref{f:architecture} shows the whole process. For each video clip, we first extract behavioral, affective, and visual feature maps. We adapt ResNet-3D~\cite{Hara2018Can} as the backbone to generate behavioral features from spacetime region. Affective and visual representations of participants are extracted using emotion recognition and face analysis tools. Then, we feed these features into an individual learning module, where features are refined by a self-attention mechanism. After that, the refined features are fed to the group learning module, which is a graph attention network learning the relationships among participants. The relationship conveys side participants' information, which helps the engagement estimation of the key elderly. Furthermore, to support the supervision, we label a real-world dataset, which is the video recording of the interaction between the elderly and an intelligent social robot, using the conventional psychological approach. The main contributions of the paper are
\begin{itemize}
  \item We propose an automated approach to analyze wild multiparty HRI videos and estimate the engagement of the elderly.
  \item We borrow the concept of engagement components from psychiatry to design our network investigating behavioral, affective, and visual features.
  \item We build a machine learning model consisting of the self-attention network and the graph attention neural network for individual and group learning, which efficiently uses both individual and group information to improve engagement estimation.
  \item We create a labeled engagement dataset from a video recording of multiparty elderly-robot interaction in a wild environment.
\end{itemize}


%This paper is organized as follows. Sec.~\ref{s:Related_Work} presents the related work of engagement estimation in HRI, especially for the elderly. Sec.~\ref{s:Engagement_Estimation_of_the_Elderly} describes our proposed approach for estimating the engagement in a real-world scenario and the method of detecting main conversation group members. In Sec.~\ref{s:Experiments_and_Results}, we elaborate the dataset we collected and used for our experiments, as well as the annotation process. The details of the implementation, evaluation metrics, and main results are also presented. Finally, Sec.~\ref{s:Conclusions} summarizes the paper and outlines possible future work.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.82\linewidth]{assets/architecture}
  \caption{Overview of the proposed engagement estimation. The method is composed of four modules: (i) Feature Extraction, (ii) Individual Learning, (iii) Group Learning, and (iv) Engagement Estimation.}
  \label{f:architecture}
\end{figure}


\begin{table*}[htb!]
  \centering
  \caption{Comparison of Engagement Estimation Methods}
  \label{t:references}
  \begin{threeparttable}[b]
  \begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}llllll}
  \toprule
  \textbf{Paper} & \textbf{Scenario} & \textbf{Participants}\tnote{1} & \textbf{Modality(s)}\tnote{2} & \textbf{Approach} \tnote{3} & \textbf{Output}\tnote{4} \\
  \midrule
  \cite{BenYoussef2019Early} & HRI & I/G & vis, aud & LR & $\hat{y} \in$ \{NBrk, Brk\} \\
  \cite{Gao2020NGage} & HHI & G (age 15-17) & phy, env & LightGBM & $\hat{y} \in$ [1, 5] \\
  \cite{Monkaresi2017Automated} & HCI & I (age 20-60) & vis & NB & $\hat{y} \in$ \{Eng, NEng\} \\
  \cite{Salam2017Fully} & HRI & M & vis, dpt, per & SVM \& RF & $\hat{y} \in$ \{Eng, NEng\} \\
  \midrule
  \cite{Anagnostopoulou2021Engagement} & HRI & I/G (children) & vis, dpt & AlexNet \& 2D CNNs & $\hat{y} \in$ \{Eng, MEng, NEng\} \\
  \cite{DelDuchetto2020Are} & HRI & I/G & vis & CNNs+LSTM & $\hat{y} \in$ [0, 1] \\
  \cite{Guhan2020ABCNet} & HHI/HCI & I & vis, aud, txt & GANs & $\hat{y} \in$ \{Eng, NEng\} \\
  \cite{Rudovic2019Personalized} & HRI & I (age 4-6) & vis & RL & $\hat{y} \in$ \{HEng, MEng, LEng\} \\
  \cite{Saleh2021Improving} & HRI & I/G & vis & I3D & $\hat{y} \in$ \{Eng, NEng\} \\
  \cite{Steinert2020Engagement} & HCI & I (PwD) & vis & LSTM & $\hat{y} \in$ \{Eng, MEng, NEng\} \\
  \cite{Sumer2021Multimodal} & HHI & G (students) & vis & MLP \& LSTM & $\hat{y} \in$ \{HEng, MEng, LEng\} \\
  \cite{Zhu2020Multirate} & HCI &
  I (age 19-27) & vis & GRU & $\hat{y} \in$ \{HEng, Eng, BEng, NEng\} \\
  \midrule
  \textbf{Ours} & HRI & M (PwD) & vis & ResNet3D+Attention+GAT & $\hat{y} \in$ [0, 1] \\
  \bottomrule
  \end{tabular*}
  \begin{tablenotes}
    \item [1] I, G, and M denote individual, group, and multiparty. The difference between multiparty and group is that multiparty treats participants separately but group treats them as a whole.
    \item [2] Modalities: vis = visual, dpt = depth, per = personality, aud = audio, phy = physiological, env = environmental, and txt = text.
    \item [3] Symbol `\&' indicates using both and comparing with each other, and symbol `+' means combining to form a framework.
    \item [4] $\hat{y}$ represents the inferred engagement label or value. For classification, Eng = Engage, Brk = Breakdown. The letters before Eng and Brk are N = Not, H = Highly, B = Barely, and M = Medium.
  \end{tablenotes}
 \end{threeparttable}
\end{table*}

% ======================= RELATED WORK ==========================
\section{Related Work}
\label{s:Related_Work}
%The estimation of engagement encompasses areas from computer vision to psychological science and psychiatric nursing.
This section briefly reviews some relevant work, especially in engagement estimation and engagement studies on geriatric psychiatry.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Engagement Estimation}
\label{subs:Automated_Engagement_Estimation}

Traditional engagement estimation~\cite{Salam2017Fully,Celiktutan2019Multimodal,BenYoussef2019Early,Monkaresi2017Automated,Gao2020NGage} extracts high-level social features, for example, body pose, facial expressions, gaze and task-related information, followed by a machine learning classifier. These features are intuitive and can be used with unimodal and multimodal combinations. Recently, with great progress of machine learning in computer vision, more and more deep learning methods have been developed for engagement estimation~\cite{Saleh2021Improving,DelDuchetto2020Are,Zhu2020Multirate,Guhan2020ABCNet,Rudovic2019Personalized,Sumer2021Multimodal, Anagnostopoulou2021Engagement}. A summary of the estimation methods is given in Table~\ref{t:references}, which also includes our proposed method for comparison.

\textbf{Machine Learning Classifiers.} In general HRI, Salam \etal~\cite{Salam2017Fully} classified engagement using support vector machine (SVM) and random forest (RF), depending on predicted personality in a triadic interaction. They advanced the concept of engagement to the group level and claimed that categorization of engagement based on individual and interpersonal features without personality is insufficient. A similar work was also proposed by Celiktutan \etal~\cite{Celiktutan2019Multimodal}. Ben-Youssef \etal~\cite{BenYoussef2019Early} studied engagement in HRI from the breakdown perspective, \ie, users leave before the expected end. They extracted nonverbal multimodal data such as the distance to the robot, gaze and head motion, facial expressions, and audio. A logistic regression (LR) classifier was used.

Another widely investigated situation is online or in-class learning. Monkaresi \etal~\cite{Monkaresi2017Automated} explored engagement in the situation where students completed an online writing activity. Heart rate, action units (AUs) and local binary patterns were extracted and fed to a set of classifiers like Naive Bayes (NB). Gao \etal~\cite{Gao2020NGage} predicted high school students' learning engagement including emotional, behavioral, and cognitive engagement in real-world classes. They used a set of features from wearable and indoor weather sensors to infer students' engagement status.

\textbf{Deep Neural Networks.} The aforementioned approaches require expert design of input features and cannot efficiently deal with large feature dimensions. Del Duchetto \etal~\cite{DelDuchetto2020Are} proposed a regression model based on CNNs and Long Short-Term Memory (LSTM) networks, which allows robots to compute the engagement from ego-view HRI videos. The model was built on a long-term dataset from an autonomous tour guide robot in a museum. Zhu \etal~\cite{Zhu2020Multirate} presented an attention-based Gated Recurrent Unit network to predict engagement of students learning online. Taking the advantage of the published dataset~\cite{BenYoussef2019Early}, Saleh \etal~\cite{Saleh2021Improving} applied Inflated 3D ConvNets architecture to predict engagement state in an end-to-end way.

To estimate the engagement of children with autism spectrum disorder interacting with robots, Anagnostopoulou \etal~\cite{Anagnostopoulou2021Engagement} compared AlexNet~\cite{Krizhevsky2012Imagenet} and 2D CNNs using 2D or 3D poses. Rudovic \etal~\cite{Rudovic2019Personalized} proposed personalized reinforcement learning (RL) to estimate engagement level (low, medium, high) from videos of child-robot interactions. The videos were labeled offline by experts, and used to personalize the policy and engagement classifier to a target child over time.

For HHI, Sumer \etal~\cite{Sumer2021Multimodal} utilized video recordings of classes to get attentional and emotional engagement features, and then applied SVM, RF, multilayer perceptron (MLP), and LSTM to predict students' engagement levels. Guhan \etal~\cite{Guhan2020ABCNet} described a multimodal GAN-based approach, called ABC-Net, to identify engagement from online dyadic HHI recordings. They utilized three-branch networks to gain valence and arousal, from which they generated engagement labels.

%%%%%%%%%%
\subsection{Engagement in Geriatric Psychiatry}
\label{subs:Engagement_for_the_Elderly}

In geriatric psychiatry, the concept and measurement of engagement is well established. For example, Cohen-Mansfield \etal proposed an Observational Method of Engagement for PwD~\cite{Cohen2009Engagement}, which was one of the most well-known tools that many studies have used to measure engagement~\cite{Trahan2014Systematic}. Following this concept, Jones \etal~\cite{Jones2018Engagement} developed the Engagement of a Person with Dementia Scale (EPWDS) towards psychosocial activities by assessing the behavioral and emotional expressions and responses. Perugia \etal~\cite{Perugia2020ENGAGEDEM} presented an affective computing framework which specifies the components of engagement in HRI.

Moreover, robotic and computer assistance has been shown to be an effective intervention. Moyle \etal~\cite{Moyle2017Use} designed a robot seal for PwD. They found that the robot seal was more effective than usual care in improving mood states and agitation and participants were more engaged with it than with a toy. Similarly, Feng \etal~\cite{Feng2021ContextEnhanced} introduced an interactive system involving a display and a robotic sheep to engage PwD. They claimed that multimodal stimuli played a significant role in promoting engagement.

However, all the previously mentioned methods require expert involvement for engagement estimation. To achieve automated estimation, Steinert \etal~\cite{Steinert2020Engagement} proposed a vanilla LSTM model to predict emotional engagement based on visual facial features (extracted by OpenFace~\cite{Baltrusaitis2018OpenFace} and VGGFace~\cite{Parkhi2015Deep}) and contextual information (daytime, wellbeing, \etc). Similar to Steinert \etal's work~\cite{Steinert2020Engagement}, our work is also an automated method. We use not only visual facial features, but also affective features, behavior features and the relation among all participants in the main interaction group.


% ==========================================
%%==========================================
\section{Concept of Engagement}
\label{subs:What_Is_Engagement}

Engagement is generally regarded as a state or a process. According to Oertel \etal~\cite{Oertel2020Engagement}, this notion is ambiguous across different domains. While in terms of the state participants are either engaged or not engaged, by process the concept emphasizes how interactors establish, maintain, and complete their perceived connection to each other during an interaction~\cite{Sidner2005Explorations}. Note that the term {\em state} represents objectively observed facts in HHI or HRI, which is whether the participants are within interaction or not. It is used to distinguish itself from \textit{process}. In this paper, we adopt Poggi's definition of engagement~\cite{Poggi2013Mind}, which refers to the participant's inner state of being together with other participants and continuing the interaction.

\subsection{Elements of Engagement}
In general, engagement contains several elements~\cite{Castellano2009Detecting,Guhan2020ABCNet,Sumer2021Multimodal,Finn2012Student, OBrien2008What,CohenMansfield2011Comprehensive,Archambault2017Joint,BenEliyahu2018Investigating,Corrigan2016Engagement,Perugia2020ENGAGEDEM}. They usually include behavioral, affective, visual, verbal, social, and cognitive signals.
\begin{itemize}
  \item \textbf{Behavioral} involves observable behaviors such as approaching, touching, avoiding, and hitting.
  \item \textbf{Affective} is defined as the reactions that are usually represented by the valence and arousal.
  \item \textbf{Visual} encompasses actions involving the eyes and head such as maintaining contact or appearing inattention to others or materials.
  \item \textbf{Verbal} reflects the sounds and semantic information towards other participants.
  \item \textbf{Cognitive} refers to psychological investment and effort allocation of the person in order to fully comprehend the situation.
  \item \textbf{Social} includes the activities of encouraging or disrupting others.
\end{itemize}
Moreover, these elements are not mutually exclusive but often overlap with each other.

In this work, our goal is to estimate the engagement of the elderly interacting with a humanoid robot in casual conversation via a computer vision approach, so we pay attention to behavioral, affective, and visual engagement. The verbal element is eliminated due to the input modality, and the cognitive and social elements are overlooked due to the participant's physical and mental conditions.


\subsection{Engagement in Different Scenarios}
Engagement estimation is studied in many disciplines and interaction scenarios. A simple taxonomy is based on the type of interactors: engagement in HHI or HRI. Although participants may behave differently in HHI and HRI, the estimation of engagement in these two disciplines is similar in terms of methodology. Thus the knowledge from the HHI could be applied to HRI~\cite{Oertel2021EngagementAware}.

In addition, the application scenarios of engagement estimation could be different. Examples are everyday conversations, healthcare and learning situations. In different scenarios, engagement may have different dominance of its elements. As a result, the corresponding estimation methods are different, and it is difficult to make fair comparison of different methods. There is no universal approach.


% ========================= APPROACH ============================
\section{Proposed Method}
\label{s:Engagement_Estimation_of_the_Elderly}

%Our problem can be described as follows. The input is raw video recordings of the elderly-robot interaction. Formally, we denote the dataset by $\mathcal{V} = \{V_1, ..., V_m, ..., V_M\}$ where $V_m$ is the video recordings of an interaction session $m$. For each interaction session, normally there is only one old participant $p$. The session consists of $K$ video clips denoted by $V_m = \{v_m^1, ..., v_m^k, ..., v_m^K\}$ where $K$ may vary from session to session. It is assumed that each video clip is associated with a value $y_m^k \in [0, 1]$ representing the elderly's engagement state in this video clip. The value 0 represents the lowest level of engagement and the value 1 represents the highest level of engagement. Our goal is to develop a method to estimate the value of engagement $\hat{y}_m^k$.

Our problem can be described as follows. The input is a raw video clip of the elderly-robot interaction in a wild multiparty environment. This video clip has a duration of about 10 seconds and contains only one elderly as the main participant and possibly a few other participants. It is assumed that within this duration the elderly's engagement state is fixed and corresponds to a value in $[0, 1]$. The value 0 represents the lowest level of engagement and the value 1 represents the highest level of engagement. Our goal is to estimate the value of engagement. In real applications, a human-robot interaction usually contains several interaction sessions and each session can be further divided into a sequence of clips. If we can estimate the value of engagement for each clip, we can obtain the engagement state over the entire interaction session.

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.98\linewidth]{assets/pipeline}
  \caption{Architecture of the proposed network. A ResNet-3D model is adapted for extracting spatio-temporal features. Multi-person tracking and multi-face detection are used to get the bounding boxes in order to align and slice out corresponding body and face feature maps, which are then pooled for individual learning. The self-attention mechanism or average pooling is applied to refine the behavioral ($\mathcal{B}$), affective ($\mathcal{A}$), and visual ($\mathcal{V}$) features. The concatenation of these learned three components gives the  representation of an individual, which is then further improved via group learning. Finally, a fully connected layer estimates the elderly's engagement state.}
  \label{f:pipeline_4}
\end{figure*}

In this section, we present a supervised learning method for estimating the value of engagement of the elderly given a video clip. The method consists of four modules.
%The overview of the method is shown in Fig.~\ref{f:architecture}.
The first module is feature extraction. We use some pre-trained network models to obtain spatio-temporal representations of the input videos, from which behavior, affective and visual face features are extracted. The second module is individual learning, which refines individual features by adding a self-attention mechanism. Because the facial features and body movements of older adults are difficult to recognize, we introduce the attention mechanism to enrich individual features. The third module is group learning. We construct a graph network to learn the response from nurses as well as the relationships of participants within the group, which further helps to understand the engagement of the elderly. The last module is engagement estimation, which generates a value representing the elderly's engagement state. This is done just simply by a fully connected (FC) estimator. That is, we treat our task as a regression problem, and let the final layer of the network be a fully connected layer.
The loss function for training is based on the mean squared error (MSE):
\begin{equation}
  M\!S\!E = \frac{1}{M}\sum_{i=1}^M(y_i-\hat{y}_i)^2,
  \label{eq:mse}
\end{equation}
where $y_i$ is the predicted engagement value, $\hat{y}_i$ is the ground truth, and $M$ is the number of video clips. Fig.~\ref{f:pipeline_4} gives the overall architecture of the proposed method. We next elaborate on the first three modules in detail.

\subsection{Feature Extraction}
\label{subs:Feature_Extraction}

We use ResNet-3D~\cite{Hara2018Can} as the backbone to capture spatio-temporal context of an input video clip. This is motivated by the promising performance of ResNet-3D models in a wide range of video-related benchmarks~\cite{Chen2021Deep}. We utilize the feature representations extracted by this backbone pre-trained on Kinetics 400~\cite{Kay2017Kinetics}. The model details are listed in Table~\ref{t:model_structure}. The output, $X^B_{r4} \in \mathbb{R}^{1024\times4\times14\times14}$ from $res_4$ layer as the spatio-temporal feature maps, represents the behavioral engagement. Here 1024 is the channel depth, 4 is the temporal dimension and $14\times14$ is the map size. As depicted in Fig.~\ref{f:pipeline_4}, we extract four feature maps in time positions from this representation.

Meanwhile, we conduct multi-person tracking on each input video clip. The purpose of this step is threefold: (i) the main interaction participants are identified based on the bounding boxes; (ii) we use these bounding boxes to eliminate the interference of redundant background information; and (iii) the tracked bounding boxes are used as constraints for face tracking as a way to ensure the consistency of face and body information.

To do this, we first perform multi-person tracking (MPT) using ByteTrack~\cite{Zhang2021ByteTrack} to gain the bounding boxes (BBX) of the detected bodies. We identify the main group members intuitively based on the frequency of a person's appearance in the temporal axis and the distance from the camera in the spatial axis. We evaluate the distance by the size of the BBX. We set the frequency threshold to be 20\% and the minimum BBX size to be 5000 pixels.

\begin{table}[htb]
  \centering
  \caption{The structures of our backbone model and expression analysis module.}
  \label{t:model_structure}
  \begin{tabular}{>{\centering}p{0.14\textwidth} >{\centering}p{0.18\textwidth} >{\centering}p{0.15\textwidth} >{\centering}p{0.18\textwidth} >{\centering\arraybackslash}p{0.15\textwidth}}
  \toprule
  \multirow{2}{*}{\textbf{Layer Name}} &
  \multicolumn{2}{c}{\textbf{Backbone}} &
  \multicolumn{2}{c}{\textbf{Expression Analysis}} \\
  \cmidrule(lr){2-3} \cmidrule(lr){4-5}
  & Architecture & Output size & Architecture & Output size \\
  \midrule
  conv1 &
  $5\times7\times7$, stride 2, 2, 2 &
  $16\times112\times112$ &
  $7\times7$, stride 2, 2 &
  $112\times112$ \\
  \midrule
  maxpool1 &
  $2\times3\times3$, stride 2, 2, 2 &
  $8\times56\times56$ &
  $2\times2$, stride 2, 2 &
  $56\times56$ \\
  \midrule
  res2 &
  $\left[ \begin{array}{c} 3\times1\times1, 64 \\ 1\times3\times3, 64 \\ 1\times1\times1, 256 \end{array}\right]\times3$ &
  $8\times56\times56$ &
  $\left[ \begin{array}{c} 3\times3, 64 \\ 3\times3, 64 \end{array}\right]\times2$ &
  $56\times56$ \\
  \midrule
  maxpool2 &
  $2\times1\times1$, stride 2, 1, 1 &
  $4\times56\times56$ &
  \multicolumn{2}{c}{N.A.} \\
  \midrule
  res3 &
  $\left[ \begin{array}{c} 3\times1\times1, 128 \\ 1\times3\times3, 128 \\ 1\times1\times1, 512 \end{array}\right]\times4$ &
  $4\times28\times28$ &
  $\left[ \begin{array}{c} 3\times3, 128 \\ 3\times3, 128 \end{array}\right]\times2$ &
  $28\times28$ \\
  \midrule
  res4 &
  $\left[ \begin{array}{c} 3\times1\times1, 256 \\ 1\times3\times3, 256 \\ 1\times1\times1, 1024 \end{array}\right]\times6$ &
  $4\times14\times14$ &
  $\left[ \begin{array}{c} 3\times3, 256 \\ 3\times3, 256 \end{array}\right]\times2$ &
  $14\times14$ \\
  \midrule
  res5 &
  \multicolumn{2}{c}{pruned} &
  $\left[ \begin{array}{c} 3\times3, 512 \\ 3\times3, 512 \end{array}\right]\times2$ &
  $7\times7$ \\
  \bottomrule
  \end{tabular}
\end{table}

Given these, we use RoI Align~\cite{He2017Mask} to project the coordinates on the frames' feature maps and slice out the corresponding features for each individual. After that the behavioral feature maps are refined to $X^B \in \mathbb{R}^{N\times1024\times4\times7\times7}$, where $N$ is the number of detected participants. Formally,
\begin{equation}
  X^B = RoI\left(E\left(v\right), {\rm BBX}\right)
\end{equation}
where $v$ represents the video clip, and $RoI$ and $E$ are the RoI align and feature extraction operations.

To extract affective and visual engagement features, we first perform multi-face detection by RetinaFace~\cite{Deng2020RetinaFace}. In order to ensure the consistency of the face and body in the temporal dimension, we keep $T = 4$ in the temporal direction. The detected $N\times4$ faces are used for affective and visual feature extraction.

Specifically, we employ a pre-trained ResNet-18 emotion recognition model, DMUE~\cite{She2021Dive}, on the cropped and aligned faces. To keep more facial information, we do not use the original 8 emotion classification results, but instead, we remove the final linear projection layer and use the mid-output from $res_5$ layer to represent affective engagement. An average pooling is used to down sample features patch, which gives the affective features $X^A \in \mathbb{R}^{N\times4\times512}$.

Also, we use OpenFace~\cite{Baltrusaitis2018OpenFace} to extract the visual features. Since visual engagement is highly related to head and eyes behaviors, we select head pose and gaze features. Particularly, 6 head pose, 6 gaze directions and 2 gaze angles, 112 two-dimensional eye region landmarks, and 168 three-dimensional landmarks are extracted, which together form the visual engagement features $X^V \in \mathbb{R}^{N\times4\times294}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Individual Learning}
\label{subs:Individual_Learning}

For the behavior features extracted in the previous module, though they are localized to the bounding boxes, they lack detailed body posture and action information, which actually plays an important role in understanding behavioral engagement. To overcome this issue, we introduce a self-attention mechanism~\cite{Wang2018NonLocal} to refine the behavioral features. We hope that the attention mechanism can learn the interaction between any two feature positions in spatio-temporal dimensions and accordingly leverage this information to improve the feature representations, \ie, focusing more on the important body regions in the spatial domain and the critical frames in the temporal domain. As demonstrated by the ablation study in Sec.~\ref{s:Experiments_and_Results}, capturing such fine details contributes to the improvement of estimation performance.

\begin{figure}[b]
  \centering
  \includegraphics[width=0.65\linewidth]{assets/self-attention}
  \caption{Self-attention block. The convolutional layers are all with kernel size of $1\times1\times1$, but have different weights.}
  \label{f:self-attention}
\end{figure}

In our implementation, the self-attention mechanism is a non-local operation, which calculates the response at a given position as a weighted sum of the features at all positions. That is, the self-attention block receives behavioral feature maps $X^B$ extracted from the previous module as input and outputs the updated representations highlighting the most informative features. The non-local block is shown in Fig.~\ref{f:self-attention}. $X^B$ is fed into three separate convolutions to embed the feature map. The non-local operation $f$, together with $g$, a simple linear embedding, computes the relationship between different locations. Then a residual connection is applied, followed by an average pooling to down sample feature maps to the size of $N\times1024$.

For affective and visual features, because of the relative small feature dimension, we do not apply self-attention mechanism on them. Instead, we simply apply an average pooling (AP), which works on the temporal dimension, to make affective and visual features have appropriate size with the behavioral features.

Finally, we concatenate the three individual's features to derive the refined individual feature map:
\begin{equation}
\label{eq:h}
  \mathbf{H} = \left[\alpha\left(X^B\right), {\rm AP}\left(X^A\right), {\rm AP}\left(X^V\right)\right].
\end{equation}
where $\alpha$ is the attention operation.

Note that in Eq.~\ref{eq:h}, $\mathbf{H}$ is represented in terms of feature types. We reorganize it according to the detected people. Without ambiguity, we still use the notation $\mathbf{H}$, \ie, $\mathbf{H} = \left[h_1, ..., h_N\right]$, where each $h_i$ contains behavioral, affective, and visual features obtained from individual learning, particularly $h_1$ is for the elderly, and $\left[\cdot, \cdot\right]$ denotes concatenation. This $\mathbf{H}$ is then used as the input to the next module: group learning.


%%%%%
\subsection{Group Learning}
\label{subs:Group_Learning}

Engagement estimation of the elderly relies on subtle interactions among individuals present in a multiparty HRI scenario. It has been known that estimating engagement solely from the elderly is not very reliable. In fact, in the elderly-nurse-robot interaction scenario, nurses are not just the auxiliaries and participants of the interaction. They are also the people who are in daily contact with the elderly and hence they have the prompt judgment about the expressions of the elderly. These judgments are conveyed in their behaviors.

Generally in human conversation, each participant plays a specific role: speaker, addressee, or side-participant who is part of the group of potential speakers but is currently taking on a listening role~\cite{Goffman1981Forms,Clark1996Using}. In a wild dynamic multiparty interaction, we define the main interaction group to be composed of these participants including speaker, addressee and side-participants. The rest, who may be bystanders and overhearers, is called the background.
In the elderly-nurse-robot interaction scenario described above, we hypothesize that \textit{analyzing all participants in the main interaction group and their relationships helps to estimate the engagement of the individual elderly.}

To represent the main interaction group, we propose to use the graph structure where each node corresponds to a participant and stores his/her feature map, and each edge represents the interaction between the participants of the two nodes. We further build graph neural networks (GNNs)~\cite{Scarselli2008Graph} to learn the graph representation, which is to compute the hidden representation of each node in the graph by attending over the rest. Specifically, we adapt a two-layer graph attention network (GAT)~\cite{Velivckovic2017Graph} to learn the underlying interactions between nodes by computing attention weights for each edge. The input to our network is $\mathbf{H} = \{h_1, ..., h_N\}$ that are derived from individual learning. The network outputs a new set of transformed node features $\mathbf{L} = \{l_1, ..., l_N\}$. Fig.~\ref{f:group_learning} illustrates the two-layer group learning process.

\begin{figure}[hb]
  \centering
  \includegraphics[width=0.72\linewidth]{assets/group_learning}
  \caption{Schematic diagram of the group learning process with two graph attention layers (GALs).}
  \label{f:group_learning}
\end{figure}

Following the approach of \cite{Velivckovic2017Graph}, we first apply a learnable transformation parameterized by a shared weight matrix $W$ to every node feature $h_i$ in order to obtain a higher-level feature $W h_i$.

Then we compute the score $e_{ij}$ of attention from node $j$ to node $i$ by
\begin{eqnarray}
\label{eq:e1j}
e_{1j} &=& \mathbf{a}_1\cdot W h_1 +   \mathbf{b}_1 \cdot W h_j \\
e_{ij} &=& \mathbf{a}_2\cdot W h_i +   \mathbf{b}_2 \cdot W h_j, \;\; \text{for}\; i \neq 1
\end{eqnarray}
where $\mathbf{a}$ and $\mathbf{b}$ are the weight vectors to be learnt, and ``$\cdot$'' represents the dot product of vectors. Here $\mathbf{a}_1$ and $\mathbf{b}_1$ are for the attention from any node to node $1$ only, and $\mathbf{a}_2$ and $\mathbf{b}_2$ are shared for all other situations. This special design is due to the fact that the elderly is the main participant among all the participants in the group.

Next we apply the LeakyReLU nonlinearity to the scores. They are further passed through a softmax operation to generate the normalized weights:
\begin{equation}
    \alpha_{ij} = \frac{{\rm exp}\left({\rm LeakyReLU}(e_{ij})\right)}{\sum_{k=1}^{N}{\rm exp}\left({\rm LeakyReLU}(e_{ik})\right)}.
\end{equation}

The normalized weights are used to compute the new node feature as a linear combination of the old features:
\begin{equation}
  \label{eq:hiprime}
  h_i^{\prime} = \sum\limits_{j=1}^{N} \alpha_{ij}W h_j.
\end{equation}

To stabilize the learning process, we employ multi-head attention, where $K (>1)$ independent attention mechanisms execute the transformation, resulting in $K$ different $h_i^{\prime}$. Then, the new features of $i$-th node ${h_i^k}^{\prime}$  ($k \in \left[1, K\right]$) from every head are passed through another LeakyReLU activation function and concatenated to derive the updated node feature $g_i$:
\begin{equation}
  g_i = \left[ {\rm LeakyReLU}({h_i^1}^{\prime}),\ ...,\ {\rm LeakyReLU}({h_i^K}^{\prime}) \right].
\end{equation}

Here, $g_i$ is the output node feature of the first graph attention layer (GAL), where the dimension of $g_i$ is $K$ times larger than that of $h_i$. We then forward this updated node features $\mathbf{G} = \{g_1, ..., g_N\}$ to the second GAL, \ie, mathematically go through Eq.~\ref{eq:e1j} to Eq.~\ref{eq:hiprime} by substituting $g$ for $h$. Moreover, a new set of learnable weights $V$, $\mathbf{c}$ and $\mathbf{d}$ are used to replace $W$, $\mathbf{a}$ and $\mathbf{b}$, which eventually gives $g_i^{\prime}$:
\begin{equation}
  g_i^{\prime} = \sum\limits_{j=1}^{N} \beta_{ij}V g_j.
\end{equation}
where $\beta_{ij}$ is the normalized attention coefficients for the second layer, which has the same function as $\alpha_{ij}$. Similarly, this second GAL also includes $K$ heads, so the final output for node $i$ of our group learning module is an average of ${g_i^k}^{\prime}$. That is
\begin{equation}
  l_i = \frac{1}{K}\sum\limits_{k=1}^K \sum\limits_{j=1}^N \beta_{ij}^k V^k g_j
\end{equation}
where $\beta_{ij}^k$ is the normalized weights computed by the $k$-th attention mechanism and $V^k$ is the corresponding weight matrix.

%\begin{figure}[t]
%  \centering
%  \includegraphics[width=0.38\linewidth]{assets/gats}
%  \caption{Schematic diagram of GATs by node 1 on its neighborhood. The aggregated %features from each head are concatenated to obtain $h_1^{\prime}$. The arrows to node 1 are from multi-head architecture.}
%  \label{f:gats}
%\end{figure}


% ======================== EXPERIMENTS ==========================
\section{Experiments and Results}
\label{s:Experiments_and_Results}

This section reports our experiments, which include dataset labeling, implementation detail, main results, and ablation studies.

\subsection{BHEH Dataset}
\label{subs:BHEH_Dataset}

To the best of our knowledge, there is no publicly available labeled dataset for learning elderly-robot interaction, not to mention that in a multiparty scenario. In our experiments, we used BHEH dataset, which was collected in a project that studied the interaction of the elderly with a socially intelligent humanoid robot at Bright Hill Evergreen Home (BHEH), Singapore.
%NTU Institutional Review Board has approved this study. A detailed consent form was signed before the onset of the procedure, followed by a detailed explanation of the experiment.
The BHEH dataset includes video recordings of real elderly-robot interaction where 29 participants aged 60 years and above with dementia participated. There was no constraint imposed on the participants. The elderly talked to a humanoid robot while the nurses occasionally provided help. Moreover, the dataset was collected in a wild, dynamic, and multiparty environment. In the background, nurses might pass through the scene; some old people might sit near to or far from the interaction group; and the interaction participants might leave and join at any time.
More detailed information of the dataset and the research setting can be found in \cite{Mishra2021Does,Tulsulkar2021Can}.

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{assets/annotation_form}
  \caption{The simplified annotation form of EPWDS.}
  \label{f:annotation_form}
\end{figure*}

\begin{figure}[thb!]
  \centering
  \includegraphics[width=\linewidth]{assets/label_overview}
  \caption{Overview of the engagement annotation. The horizontal axis and vertical axis represent the EPWDS engagement value and the video frame count, respectively.}
  \label{f:label_overview}
\end{figure}

We manually annotated 43 interaction sessions. The length of the videos are between 3 and 38 minutes (over 560 minutes in total). The number of participants for each session is from 1 to 6.

The label of the data is the engagement score, which was obtained by normalizing the EPWDS values~\cite{Jones2018Engagement} to $[0, 1]$. We actually removed two components, which were less relevant to our problem, to simplify the process of EPWDS for artificial engagement annotation. The detail of the annotation form is shown in Fig.~\ref{f:annotation_form}. Each interaction session was annotated at least by two experts. Fig.~\ref{f:label_overview} illustrates the labelled engagement statistics.

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation Details}
\label{subs:Implementation_Details}

The original BHEH videos were collected in 15 fps. To extract video features by the pre-trained networks, we sampled the videos by selecting one frame from every 5 frames and set 32 frames as the clip length. As a result, each video clip records the interaction for about 10.67 seconds, which were used as the input to our method.

We randomly split our dataset into 5 subsets based on interaction sessions under the constraint that ensures that no interaction session appearing in two sets. We conducted 5-fold cross test to evaluate our model, \ie, we used one set as testing set and the rest as training set in every fold. The reported results are the average error.

In our model, we adapted a pretrained ResNet50-3D as the backbone, followed by RoI Align to crop the behavior feature map into size of $7\times7$. We performed the self-attention module on individuals' behavior representations by a non-local block with embedded Gaussian bottleneck. Affective and visual features were gained through DMUE~\cite{She2021Dive} module and OpenFace~\cite{Baltrusaitis2018OpenFace} toolkit. The concatenated individuals' feature maps were fed into a two-layer, 3-head GATs module with hidden size 64, dropout rate 0.5, and slope $\alpha = 0.2$. We trained our model in two stages. The individual learning module was trained first, and then we fine-tuned the network end-to-end including the adapted GAT module. Both stages were trained using Adam optimizer in 80 epochs with initial learning rate $10^{-4}$, divided by 10 every 40 epochs. The MSE loss is used in the training process.

\subsection{Results}
\label{subs:Main_Results}

We attempted to compare our proposed method with the state-of-the-art methods in engagement estimation. Note that there are no publicly available datasets and benchmarks for our problem. Moreover, the scenarios of HRI involved in the state-of-the-art are very different and the results on different datasets may appear different. Hence, it is very difficult to perform a fair comparison. We took a compromise approach by applying the prior art methods to our dataset. It should be pointed out that the prior art methods we compared with do not publish their codes, so we implemented them ourselves.

For 2D CNNs~\cite{Anagnostopoulou2021Engagement}, as our inputs were just RGB videos without depth information, we adopted its 2D body pose as features and used AlexNet as the comparison model, which achieved similar performance reported in its paper. Inception V2~\cite{Saleh2021Improving} had the same input format as ours, so the implementation just followed its light-inception model architecture. In \cite{Steinert2020Engagement}, LSTM was used to classify engagement based on extracted OpenFace and VGG features. To make fair comparisons, we used the same frame length. For evaluation, we used two metrics, MSE of Eq.~\ref{eq:mse} and mean absolute error (MAE) defined as follows:
\begin{equation}
  M\!A\!E = \frac{1}{M}\sum_{i=1}^M|y_i-\hat{y}_i|
\end{equation}
where $y_i$ is the predicted engagement value, $\hat{y}_i$ is the ground truth, and $M$ is the number of sample video clips.

The results are reported in Table~\ref{t:main_results}. The testing losses are shown in Fig.~\ref{f:loss}. The performance of our method (with MSE=0.0148) outperforms the prior art. For better understanding, we provide three estimation examples in Fig.~\ref{f:visualization} which shows the center frame of the example clip, group detection, behavioral representation from body align, affective and visual representation, and estimation results from left column to right column. It can be seen that our method achieves good results` even under challenging conditions. Particularly, in examples~1 and~5, participants' bodies are detected and used for feature extraction, but the leftmost person is not desired. This is a counterexample of the main group detection. In contrast, examples~2 and~3 detect all the participants successfully. In terms of affective and visual engagement representation, some inconsistency and instability occur due to the masks and senescent faces. For instance, the elderly in example 1 could not make meaningful expressions and the visual features in example 3 are also missed. This may explain why those methods only involving facial information often fail to produce good results. In addition, the elderly from examples~1 and~3 is not good at body language, so the information from side participants helps in the estimation, \eg, the body representation captures the raised hand of the nurse.

\begin{table}[hbt]
  \vskip -0.3cm
  \centering
  \caption{Engagement estimation of the elderly.}
  \label{t:main_results}
  \begin{tabular}{ccc}
  \toprule
  \textbf{} & \textbf{MSE} & \textbf{MAE} \\
  \midrule
  Random Guess & 0.1763 & 0.3435 \\
  2D CNNs~\cite{Anagnostopoulou2021Engagement} & 0.1427 & 0.3711 \\
  Inception V2~\cite{Saleh2021Improving} & 0.0283 & 0.1649 \\
  LSTM~\cite{Steinert2020Engagement} & 0.1148 & 0.3030 \\
  \midrule
  \textbf{Ours}& \textbf{0.0148} & \textbf{0.0996} \\
  \bottomrule
  \end{tabular}
\end{table}
\vskip -0.8cm

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.55\linewidth]{assets/loss}
  \caption{Losses of MSE and MAE on the testing set.}
  \label{f:loss}
\end{figure}
\vskip -0.8cm

\begin{figure}[hbt!]
  \centering
  \includegraphics[width=0.85\linewidth]{assets/visualization}
  \caption{Visualization of the engagement estimation results.}
  \label{f:visualization}
\end{figure}

\subsection{Ablation Studies}
\label{subs:Ablation_Studies}

We ran a number of ablations to analyze our method. The results are reported in Table~\ref{t:ablations}. $\mathcal{B}$, $\mathcal{A}$, and $\mathcal{V}$ are the results of using a single engagement element of behavioral, affective, and visual. It can be seen that the results are inferior to that produced by the proposed multi-element method. The self-attention module also helps improve the performance by 0.0105 in MSE and 0.0386 in MAE. By employing the GATs in our group learning module, the results have 0.0148 increase in MSE, which means that the signals from side-participants contribute to the estimation in our multiparty elderly-robot interaction. For the comparison of ordinary GAL and our adapted GAL, we found that the adapted model achieves better results, although this performance improvement is not very significant. Fig.~\ref{f:gal_vs_adpgal} illustrates the the MSE and MAE results of these two kinds of graph layer.

\begin{table}[hb]
  \centering
  \caption{Ablation results.}
  \label{t:ablations}
  \begin{tabular}{ccc}
  \toprule
  \textbf{} & \textbf{MSE} & \textbf{MAE} \\
  \midrule
  $\mathcal{B}$ & 0.0451 ($\downarrow$0.0303) & 0.1750 ($\downarrow$0.0754) \\
  $\mathcal{A}$ & 0.1235 ($\downarrow$0.1087) & 0.3690 ($\downarrow$0.2694) \\
  $\mathcal{V}$ & 0.1567 ($\downarrow$0.1419) & 0.4184 ($\downarrow$0.3188) \\
  w/o self-attention & 0.0253 ($\downarrow$0.0105) & 0.1382 ($\downarrow$0.0386) \\
  w/o GATs & 0.0296 ($\downarrow$0.0148) & 0.1380 ($\downarrow$0.0384) \\
  original GATs & 0.0173 ($\downarrow$0.0025) & 0.1080 ($\downarrow$0.0084) \\
  \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[hb]
  \centering
  \includegraphics[width=0.6\linewidth]{assets/gal_vs_adpgal}
  \caption{Comparison between ordinary GAL and our adapted GAL in terms of the MSE and MAE losses.}
  \label{f:gal_vs_adpgal}
\end{figure}

% ======================== CONCLUSIONS ==========================
\section{Conclusions}
\label{s:Conclusions}
This paper has presented an automatic method for analyzing wild multiparty human-robot interaction (HRI) videos and estimating the engagement state of the elderly--the main participant--in the HRI. The method adapts pre-trained models to extract behavioral, affective and visual features of the participants in the main interaction group from real-world videos of such interactions. A supervised machine learning model consisting of a self-attention network for individual learning and a graph attention network for group learning is designed to take these multi-modal features of all participants as input and output the predicted engagement state of the elderly. To support the supervision, we have created a labeled engagement dataset from a video recording of multiparty elderly-robot interaction in a wild environment. By utilizing multi-modal features and exploiting individual and group learning, our method can effectively predict the engagement and outperform the prior art, as confirmed by the experimental results.



%\backmatter

%\section*{Acknowledgments}
%This is acknowledgment text. Provide text here.

%\subsection*{Author contributions}

%This is an author contribution text.

%\subsection*{Financial disclosure}

%None reported.

%\subsection*{Conflict of interest}

%The authors declare no potential conflict of interests.


%\section*{Supporting information}

%The following supporting information is available as part of the online article:
%\noindent
%\textbf{Figure S1.}
%{500{\uns}hPa geopotential anomalies for GC2C calculated against the ERA Interim reanalysis. The period is 1989--2008.}

%\noindent
%\textbf{Figure S2.}
%{The SST anomalies for GC2C calculated against the observations (OIsst).}


% ======================== APPENDICES ===========================
%\section{Appendix}
%\label{ap:annotation_form}

% ======================== REFERENCES ===========================

%\nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;
\bibliography{cavw}%

%\section*{Author Biography}

%begin{biography}{\includegraphics[width=66pt,height=86pt,draft]{empty}}{\textbf{Author Name.} %This is sample author biography text .}
%\end{biography}

\end{document}
